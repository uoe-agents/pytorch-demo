{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UoE RL Course: Lab - PyTorch Demo\n",
    "\n",
    "In this lab session, we will give you a brief introduction into PyTorch. PyTorch is widely used library for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics\n",
    "\n",
    "### Tensors\n",
    "\n",
    "At the heart of PyTorch and similar libraries are tensors. Think of a tensor as an arbitrarily-dimensional matrix. These tensors are used to apply various operations and most objects we apply operations on within PyTorch will be tensors.\n",
    "\n",
    "There are multiple ways of creating tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specific tensor\n",
    "t1 = torch.tensor(\n",
    "    [\n",
    "        [1., -1.],\n",
    "        [1., -1.]\n",
    "    ]\n",
    ")\n",
    "\n",
    "t2 = torch.tensor(np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6]\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 1., -1.],\n",
      "        [ 1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# shapes of tensors show the dimensionality:\n",
    "print(t1.shape)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(t2.shape)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# create tensor of specific shape with all 0s\n",
    "t_zeros = torch.zeros((2, 3, 2))\n",
    "print(t_zeros.shape)\n",
    "print(t_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# create tensor of specific shape with all 1s\n",
    "t_ones = torch.ones((2, 2))\n",
    "print(t_ones.shape)\n",
    "print(t_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2]) tensor([[[-0.0981,  1.6531],\n",
      "         [-0.6422,  0.5587]],\n",
      "\n",
      "        [[ 0.8772,  0.4926],\n",
      "         [-0.1100, -1.3531]],\n",
      "\n",
      "        [[-1.4381, -1.1567],\n",
      "         [ 0.3895,  1.1590]]])\n",
      "torch.Size([6, 2]) tensor([[-0.0981,  1.6531],\n",
      "        [-0.6422,  0.5587],\n",
      "        [ 0.8772,  0.4926],\n",
      "        [-0.1100, -1.3531],\n",
      "        [-1.4381, -1.1567],\n",
      "        [ 0.3895,  1.1590]])\n"
     ]
    }
   ],
   "source": [
    "# create tensors of specific shape with values from normal distribution with mean 0 and variance 1\n",
    "t_random = torch.randn((3, 2, 2))\n",
    "print(t_random.shape, t_random)\n",
    "\n",
    "t_random = t_random.view(6, 2)\n",
    "print(t_random.shape, t_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3320)\n",
      "tensor([ 1.5551, -0.0834,  1.3698, -1.4631, -2.5948,  1.5485])\n",
      "tensor([-0.1703,  0.2256])\n"
     ]
    }
   ],
   "source": [
    "# There are many operations which can be executed on top of tensors\n",
    "# Either global ...\n",
    "t_sum = t_random.sum()\n",
    "print(t_sum)\n",
    "\n",
    "# or per dimension\n",
    "t_sum = t_random.sum(dim=-1)\n",
    "print(t_sum)\n",
    "\n",
    "t_mean = t_random.mean(dim=0)\n",
    "print(t_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[ 0.5385,  0.0531],\n",
      "        [-0.6123,  0.3731],\n",
      "        [-0.6919,  0.2427]])\n",
      "tensor([[4.5385, 4.0531],\n",
      "        [3.3877, 4.3731],\n",
      "        [3.3081, 4.2427]])\n",
      "tensor([[0.3887, 0.5845],\n",
      "        [0.7201, 0.9441],\n",
      "        [0.3686, 2.1444]])\n",
      "tensor([[1.7641, 2.3689],\n",
      "        [2.4394, 4.1285],\n",
      "        [1.2193, 9.0980]])\n"
     ]
    }
   ],
   "source": [
    "# We can also just calculate with them\n",
    "t = torch.ones(3,2)\n",
    "print(t)\n",
    "\n",
    "# scalar multiplication\n",
    "t = t * 4\n",
    "print(t)\n",
    "\n",
    "t_random = torch.randn(3,2)\n",
    "print(t_random)\n",
    "\n",
    "# addition of tensors\n",
    "t = t + t_random\n",
    "print(t)\n",
    "\n",
    "t2 = torch.randn(3,2)\n",
    "print(t2)\n",
    "\n",
    "# element-wise multiplication of tensors (of same shape)\n",
    "t3 = t * t2\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers some of the most important fundamentals of PyTorch. We strongly suggest you to have a look at the following resources of PyTorch, including detailed documentation as well as excellent introductory tutorials available about PyTorch.\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "- **PyTorch Documentation** - https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "    PyTorch has an excellent documentation of functions implemented within the library. We encourage you to read up on functionality and make yourself more familiar with the library.\n",
    "\n",
    "\n",
    "\n",
    "- **PyTorch Tutorials** - https://pytorch.org/tutorials/\n",
    "\n",
    "    PyTorch provides tutorials on their webpage which explain more features of the library in further detail.\n",
    "    \n",
    "    Examples for good introductory tutorials:\n",
    "    - Deep learning with PyTorch: A 60 Minute Blitz - https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "    - Learning PyTorch with examples - https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "\n",
    "After talking about tensors, the objects underlying PyTorch, and some basic operations, we want to introduce you to the deep learning functionality of PyTorch.\n",
    "\n",
    "### Defining a model\n",
    "First, we have to **define a neural network** architecture. We can make use of a portfolio of pre-defined architectures or define our own operations and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class FCNetwork(nn.Module):\n",
    "    \"\"\"Fully PyTorch neural network class\n",
    "\n",
    "    :attr input_size (int): dimensionality of input tensors\n",
    "    :attr out_size (int): dimensionality of output tensors\n",
    "    :attr layers (torch.nn.Module): neural network as sequential network of multiple layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims: Iterable[int], output_activation: nn.Module = None):\n",
    "        \"\"\"Creates a network using ReLUs between layers and no activation at the end\n",
    "\n",
    "        :param dims (Iterable[int]): tuple in the form of (IN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE2,\n",
    "            ..., OUT_SIZE) for dimensionalities of layers\n",
    "        :param output_activation (nn.Module): PyTorch activation function to use after last layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = dims[0]\n",
    "        self.out_size = dims[-1]\n",
    "        \n",
    "        mods = []\n",
    "        for i in range(len(dims) - 2):\n",
    "            mods.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            mods.append(nn.ReLU())\n",
    "\n",
    "        mods.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        if output_activation:\n",
    "            mods.append(output_activation())\n",
    "        \n",
    "        self.layers = nn.Sequential(*mods)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Computes a forward pass through the network\n",
    "\n",
    "        :param x (torch.Tensor): input tensor to feed into the network\n",
    "        :return (torch.Tensor): output computed by the network\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.activation import LogSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter configuration\n",
    "\n",
    "We define a configuration dictionary that will contain our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"hidden_dims\": (28*28, 128, 128, 10),\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 3e-4,\n",
    "    \"log_interval\": 500,\n",
    "    \"epochs\": 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a fully connected network with ReLU activations and a LogSoftmax output layer. You can experiment with changing the architecture above. Also, how would the loss function change if you had different output layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FCNetwork(config[\"hidden_dims\"], output_activation=LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (5): LogSoftmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate that loads the MNIST dataset\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "      '~/files/',\n",
    "      train=True,\n",
    "      download=True,\n",
    "      transform=torchvision.transforms.Compose([\n",
    "          torchvision.transforms.ToTensor(),\n",
    "          torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "          torchvision.transforms.Lambda(lambda x: torch.flatten(x))\n",
    "    ])),\n",
    "    batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('~/files/', train=False, download=True,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            torchvision.transforms.Lambda(lambda x: torch.flatten(x))\n",
    "    ])),\n",
    "    batch_size=config[\"batch_size\"], shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some examples from the test set\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (x, y) = next(examples)\n",
    "\n",
    "# and plot them along with their labels\n",
    "fig = plt.figure()\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(x[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(\"Label: {}\".format(y[i]))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an optimiser. We are using Adam and we supply it with the parameters of the fully connected network we created\n",
    "optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We are now ready to train our model. We will be using \"nll_loss\" which stands for Negative Log Likelihood. The \"epochs\" are how many times we will iterate over all our training dataset, but the batch_size defines how many samples will pass at once through our network. We print the loss at each epoch - but should we log more things too? How can we see if gradients pass correctly through our network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francelico/miniconda3/envs/sb3/lib/python3.10/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, loss: 0.37144312262535095\n",
      "Train Epoch: 1, loss: 0.15935790538787842\n",
      "Train Epoch: 2, loss: 0.11021707952022552\n",
      "Train Epoch: 3, loss: 0.08408975601196289\n",
      "Train Epoch: 4, loss: 0.06891424208879471\n",
      "Train Epoch: 5, loss: 0.055737532675266266\n",
      "Train Epoch: 6, loss: 0.04558186233043671\n",
      "Train Epoch: 7, loss: 0.038674257695674896\n",
      "Train Epoch: 8, loss: 0.032121896743774414\n",
      "Train Epoch: 9, loss: 0.0270842257887125\n",
      "Train Epoch: 10, loss: 0.023413997143507004\n",
      "Train Epoch: 11, loss: 0.01899632252752781\n",
      "Train Epoch: 12, loss: 0.01673775725066662\n",
      "Train Epoch: 13, loss: 0.01426181010901928\n",
      "Train Epoch: 14, loss: 0.012904437258839607\n",
      "Train Epoch: 15, loss: 0.01063623372465372\n",
      "Train Epoch: 16, loss: 0.009905903600156307\n",
      "Train Epoch: 17, loss: 0.008941451087594032\n",
      "Train Epoch: 18, loss: 0.00798572413623333\n",
      "Train Epoch: 19, loss: 0.007197989150881767\n"
     ]
    }
   ],
   "source": [
    "log_interval = config[\"log_interval\"]\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    losses_epoch = []\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = net(x)\n",
    "        loss = F.nll_loss(out, y)\n",
    "        losses_epoch.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_mean = torch.mean(torch.Tensor(losses_epoch))\n",
    "    print(f\"Train Epoch: {epoch}, loss: {loss_mean}\")\n",
    "    train_losses.append(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can see what our network learned. We do a forward pass of some examples from the test set, and now we display the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (x, y) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    y = torch.argmax(net(x[i]))\n",
    "    plt.imshow(x[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(\"Prediction: {}\".format(y))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure accuracy\n",
    "\n",
    "We can also measure the accuracy of the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9792, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Measuring accuracy on the test set\n",
    "test_accuracy = 0\n",
    "for batch_id, (x, y) in enumerate(test_loader):\n",
    "    y_pred = torch.argmax(net(x), dim=-1)\n",
    "    acc = (y_pred == y).double()\n",
    "    test_accuracy += torch.sum(acc) / len(test_loader.dataset)\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e514887b9b96f70572a73cf62746e342032877885c78cb6abc1e2e4a3e4bb8da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
